{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog and Cat Dataset을 가지고 영상처리 실습\n",
    "\n",
    "Download Cat and Dog Dataset: https://www.kaggle.com/c/dogs-vs-cats/data 에서 다운로드 받기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib.pyplot import *\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "# use matplotlib as inline\n",
    "%matplotlib inline \n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cat_and_dog_files(data):\n",
    "\n",
    "    cat_files = []\n",
    "    dog_files = []\n",
    "\n",
    "    for string in data:\n",
    "\n",
    "        result1 = re.search(\"cat\", string)\n",
    "        result2 = re.search(\"dog\", string)\n",
    "        \n",
    "        if result1 is not None:\n",
    "            cat_files.append(string)\n",
    "        if result2 is not None:\n",
    "            dog_files.append(string)   \n",
    "\n",
    "    return cat_files, dog_files      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 images found:\n"
     ]
    }
   ],
   "source": [
    "import os        # os에 관계된 라이브러리들을 import한다.  \n",
    "import glob\n",
    "dirname = 'data/train'   # 디렉토리명\n",
    "dataset_path = \"data/train/*.jpg\"\n",
    "#dataset_path = \"*.jpg\"\n",
    "data = glob.glob(dataset_path)\n",
    "\n",
    "# dirname이라는 디렉토리 안에 있는 모든 파일을 filenames에 저장한다. \n",
    "filenames = [os.path.join(dirname, fname)   # 밑에서 얻은 파일들을 [](배열)안에 추가 \n",
    "             for fname in os.listdir(dirname)] \n",
    "#print(filenames)\n",
    "#data = glob.glob(dirname)\n",
    "print(\"{} images found:\".format(len(data)))\n",
    "#print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test image import\n",
    "dirname_test = 'data/test'\n",
    "dataset_path2 = \"data/test/*.jpg\"\n",
    "data_test = glob.glob(dataset_path2)\n",
    "\n",
    "# dirname이라는 디렉토리 안에 있는 모든 파일을 filenames에 저장한다. \n",
    "filenames_test = [os.path.join(dirname_test, fname)   # 밑에서 얻은 파일들을 [](배열)안에 추가 \n",
    "             for fname in os.listdir(dirname_test)] \n",
    "#print(filenames)\n",
    "#data = glob.glob(dirname)\n",
    "print(\"{} images found:\".format(len(data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filenames섞기\n",
    "import random\n",
    "\n",
    "random.shuffle(filenames)\n",
    "#print(filenames)\n",
    "random.shuffle(filenames)\n",
    "random.shuffle(filenames)\n",
    "#print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# label만들기\n",
    "import re\n",
    "\n",
    "def get_labels(data):\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for string in data:\n",
    "\n",
    "        result1 = re.search(\"cat\", string)\n",
    "        result2 = re.search(\"dog\", string)\n",
    "        \n",
    "        if result1 is not None:\n",
    "            labels.append([1.0, 0.0])\n",
    "        if result2 is not None:\n",
    "            labels.append([0.0, 1.0])\n",
    "\n",
    "    return labels\n",
    "\n",
    "labels = get_labels(filenames) # label 만들기\n",
    "labels_test = get_labels(filenames_test) # test label 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import misc # feel free to use another image loader\n",
    "\n",
    "batch_size = 50\n",
    "total = 24900\n",
    "total_batch = int(total/batch_size)\n",
    "\n",
    "def create_batches(list_of_images,labels,batch_size):\n",
    "    images = []\n",
    "    img_size = (224, 224, 3)\n",
    "    for img in list_of_images:\n",
    "        images.append(misc.imresize(misc.imread(img),img_size))\n",
    "    \n",
    "    images = np.asarray(images)\n",
    "    images = images.astype(np.float32)\n",
    "    images[:,:,:,0] /= 255.    \n",
    "    images[:,:,:,1] /= 255.  \n",
    "    images[:,:,:,2] /= 255.    \n",
    "    images[:,:,:,0] -= 0.5    \n",
    "    images[:,:,:,1] -= 0.5\n",
    "    images[:,:,:,2] -= 0.5\n",
    "    images[:,:,:,0] *= 2.0    \n",
    "    images[:,:,:,1] *= 2.0\n",
    "    images[:,:,:,2] *= 2.0\n",
    "    \n",
    "    while (True):\n",
    "        for i in range(0,total,batch_size):\n",
    "            yield(images[i:i+batch_size],labels[i:i+batch_size])\n",
    "            \n",
    "def create_test_images(list_of_images):\n",
    "    images = []\n",
    "    img_size = (224, 224, 3)\n",
    "    for img in list_of_images:\n",
    "        images.append(misc.imresize(misc.imread(img),img_size))\n",
    "    \n",
    "    images = np.asarray(images)\n",
    "    images = images.astype(np.float32)\n",
    "    images[:,:,:,0] /= 255.    \n",
    "    images[:,:,:,1] /= 255.  \n",
    "    images[:,:,:,2] /= 255.    \n",
    "    images[:,:,:,0] -= 0.5    \n",
    "    images[:,:,:,1] -= 0.5\n",
    "    images[:,:,:,2] -= 0.5\n",
    "    images[:,:,:,0] *= 2.0    \n",
    "    images[:,:,:,1] *= 2.0\n",
    "    images[:,:,:,2] *= 2.0\n",
    "    \n",
    "    return images\n",
    "\n",
    "test_images = create_test_images(filenames_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loss 함수\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))\n",
    "\n",
    "#cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "#cross_entropy = tf.reduce_mean((y-y_)**2)\n",
    "\n",
    "#checkpoint를 저장하는 위치\n",
    "# ckpt = tf.train.get_checkpoint_state('./cnn')\n",
    "# global_variables 함수를 통해 앞서 정의하였던 변수들을 저장하거나 불러올 변수들로 설정함.\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "\n",
    "number_of_epochs = 10000\n",
    "#batch normalization을 제대로 하기 위해 graph에 mean average를 update하라고 말한다.\n",
    "# http://ruishu.io/2016/12/27/batchnorm/ 를 참조하기 \n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.control_dependencies(update_ops):\n",
    "     # Ensures that we execute the update_ops before performing the train_step\n",
    "     train_step = tf.train.AdamOptimizer(0.001).minimize(cost, global_step=global_step)\n",
    "    \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "\n",
    "#checkpoint가 있으면 restore하고 아니면 변수들을 초기화한다. \n",
    "#    if ckpt:\n",
    "#        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "#    else:\n",
    "#        sess.run(tf.global_variables_initializer())\n",
    "    random.shuffle(filenames)\n",
    "    labels = get_labels(filenames) # label 만들기\n",
    "    batch_generator = create_batches(filenames,labels,batch_size)\n",
    "    \n",
    "    total_cost=0\n",
    "    start_time = time.time()\n",
    "    for i in range(total_batch):\n",
    "        images, labels2 = next(batch_generator)\n",
    "        _, cost_val = sess.run([train_step,cost],\n",
    "                feed_dict={x : images, \n",
    "                           y_: labels2, \n",
    "                           phase: True})    \n",
    "        total_cost += cost_val\n",
    "    \n",
    "    print('Epoch:', '%04d' % (epoch + 1),\n",
    "          'Avg. cost =', '{:.3f}'.format(total_cost))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "  \n",
    "    acc = sess.run(accuracy, feed_dict={x: test_images, y_: labels_test, phase: False})\n",
    "    print('Accuracy:', acc) # training batch에 대한 정확도 출력....(batch사이즈가 작아서 정확하지 않아도 됨.)\n",
    "    \n",
    "#변수를 checkpoint파일에 저장.    \n",
    "#    saver.save(sess, './cnn/dnn.ckpt', global_step=global_step)\n",
    "\n",
    "print('End of optimization!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Noisy Image 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# noise영상을 사용하려면 이것을 하라.\n",
    "img_noise = np.random.uniform(size=(224,224,3)) + 100.0\n",
    "img_4d = img_noise[np.newaxis]\n",
    "# Rescale to 0-1 range\n",
    "img_4d = img_4d / np.max(img_4d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "img0 = PIL.Image.open('pilatus224.jpg')\n",
    "img0 = np.float32(img0)\n",
    "img_4d = img0[np.newaxis]\n",
    "img_4d = img_4d / 255.0\n",
    "img_4d_0 = img_4d/255\n",
    "img_4d[:,:,:,0] /= 255.    \n",
    "img_4d[:,:,:,1] /= 255.  \n",
    "img_4d[:,:,:,2] /= 255.    \n",
    "img_4d[:,:,:,0] -= 0.5    \n",
    "img_4d[:,:,:,1] -= 0.5\n",
    "img_4d[:,:,:,2] -= 0.5\n",
    "img_4d[:,:,:,0] *= 2.0    \n",
    "img_4d[:,:,:,1] *= 2.0\n",
    "img_4d[:,:,:,2] *= 2.0\n",
    "#img_4d = img_4d_0 / np.max(img_4d_0)\n",
    "#img_4d_0 = img_4d_0\n",
    "plt.imshow(visstd(img_4d[0]))\n",
    "print(img_4d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visstd(a, s=0.1):\n",
    "    '''Normalize the image range for visualization'''\n",
    "    return (a-a.mean())/max(a.std(), 1e-4)*s + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_image_grid(images_files):\n",
    "    # figure size\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # load images\n",
    "    images = [tf_keras.preprocessing.image.load_img(img) for img in images_files]\n",
    "    \n",
    "    # plot image grid\n",
    "    for x in range(4):\n",
    "        for y in range(4):\n",
    "            ax = fig.add_subplot(4, 4, 4*y+x+1)\n",
    "            plt.imshow(images[4*y+x])\n",
    "            plt.xticks(np.array([]))\n",
    "            plt.yticks(np.array([]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_image_grid(cat_files[:16])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
